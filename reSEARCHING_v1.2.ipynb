{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T18:38:38.138206Z",
     "start_time": "2025-03-13T18:38:38.133644Z"
    }
   },
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T18:38:42.919325Z",
     "start_time": "2025-03-13T18:38:38.365518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "summarization_model = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "id": "86702de3b92987cd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T18:38:42.931489Z",
     "start_time": "2025-03-13T18:38:42.925329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_text(pdf_file):\n",
    "    \"\"\"Extract text from PDF using PyPDF2.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_file, 'rb') as f:\n",
    "            pdfReader = PyPDF2.PdfReader(f, strict=False)\n",
    "            pdf_text = []\n",
    "            for page in pdfReader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    text = re.sub(r'[^\\x20-\\x7E]', ' ', text)  # Remove non-ASCII\n",
    "                    text = re.sub(r'[\\d]', ' ', text)\n",
    "                    pdf_text.append(text.strip())\n",
    "            return \" \".join(pdf_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\""
   ],
   "id": "60adb9ff6bb2e0bd",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T18:38:42.949944Z",
     "start_time": "2025-03-13T18:38:42.945810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_keywords(text, top_n=5):\n",
    "    \"\"\"Extracts top N keywords based on word frequency.\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    common_words = Counter(words).most_common(top_n)\n",
    "    return [word for word, _ in common_words]"
   ],
   "id": "f892b31b6cd08691",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T18:38:42.972750Z",
     "start_time": "2025-03-13T18:38:42.964954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def truncate_text(text, max_tokens=500):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)"
   ],
   "id": "f3696ff783397560",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T18:38:43.007349Z",
     "start_time": "2025-03-13T18:38:43.001604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(pdf_file):\n",
    "    \"\"\"Preprocesses PDF text and builds FAISS in memory (no saving).\"\"\"\n",
    "    pdf_text = get_text(pdf_file)\n",
    "    pdf_text = truncate_text(pdf_text)\n",
    "    \n",
    "    keywords = extract_keywords(pdf_text, top_n=10)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "    docs = splitter.split_text(pdf_text)\n",
    "    \n",
    "    documents = [Document(page_content=text) for text in docs if any(kw in text.lower() for kw in keywords)]\n",
    "    \n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    \n",
    "    text_embeddings_zip = list(zip(texts, embeddings))\n",
    "    \n",
    "    vector_db = FAISS.from_embeddings(text_embeddings_zip, embedding_model)\n",
    "    return vector_db"
   ],
   "id": "89288a65dc151a23",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T18:39:58.838479Z",
     "start_time": "2025-03-13T18:39:58.833496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_llm(question, pdf_file):\n",
    "    vector_db = preprocess(pdf_file)\n",
    "    \n",
    "    print(\"Question: \", question)\n",
    "    \n",
    "    # Encode the question into an embedding\n",
    "    question_embedding = embedding_model.encode([question])[0]\n",
    "    \n",
    "    # Use the correct method to query the FAISS index\n",
    "    relevant_docs = vector_db.similarity_search_by_vector(question_embedding, k=5)\n",
    "    \n",
    "    context = \" \".join([doc.page_content for doc in relevant_docs])\n",
    "    keywords = extract_keywords(context, top_n=7)\n",
    "    filtered_sentences = [sent for sent in context.split(\". \") if any(kw in sent.lower() for kw in keywords)]\n",
    "    refined_context = \" \".join(filtered_sentences)\n",
    "    \n",
    "    if not refined_context.strip():\n",
    "        refined_context = context  \n",
    "    \n",
    "    prompt = f\"Summarize the following text: {refined_context}\"\n",
    "    output = summarization_model(prompt, max_length=200, min_length=40, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    return re.sub(r'\\s+', ' ', output).strip()"
   ],
   "id": "4db8c449801f6b19",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T18:40:11.824045Z",
     "start_time": "2025-03-13T18:39:59.251335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"What is the significance of using a unified representation in the 'One Model for All' framework?\",\n",
    "    \"How does the 'One Model for All' framework handle tasks with limited data?\",\n",
    "    \"What role do modality-specific sub-networks play in the 'One Model for All' framework?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"Answer:\", ask_llm(question, \"test-1.pdf\"))"
   ],
   "id": "306cb333ea20ff0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "Your max_length is set to 200, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is the significance of using a unified representation in the 'One Model for All' framework?\n",
      "Answer: A single model yields good results on a number of problems span - ning multiple multiple domains in particular. This single model is trained concurrently on imagenet, multiple we train on. Even if a block is not crucial for a task, we observe that adding it - gated layers each of these computational blocks is crucial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "Your max_length is set to 200, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How does the 'One Model for All' framework handle tasks with limited data?\n",
      "Answer: A single model is trained concurrently on imagenet, multiple domains. It contains convolutional layers, an attention mechanism, and sparsely - gated present a single model that yields good results on a number of problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "Your max_length is set to 200, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What role do modality-specific sub-networks play in the 'One Model for All' framework?\n",
      "Answer: This model is trained concurrently on imagenet, multiple multiple domains. It contains convolutional layers, an attention mechanism, and sparsely - gated -gated layers. Each of these computational blocks is crucial for a subset of the tasks we train.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cdae8d1af2f71194"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
