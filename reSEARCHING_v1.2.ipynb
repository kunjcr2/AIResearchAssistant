{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:38:58.285184600Z",
     "start_time": "2025-03-13T04:36:04.631447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "summarization_model = pipeline(\"summarization\", model_name=\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "id": "86702de3b92987cd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:38:58.344190100Z",
     "start_time": "2025-03-13T04:36:14.400666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_text(pdf_file):\n",
    "    \"\"\"Extract text from PDF using PyPDF2.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_file, 'rb') as f:\n",
    "            pdfReader = PyPDF2.PdfReader(f, strict=False)\n",
    "            pdf_text = []\n",
    "            for page in pdfReader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    text = re.sub(r'[^\\x20-\\x7E]', ' ', text)  # Remove non-ASCII\n",
    "                    text = re.sub(r'[\\d]', ' ', text)\n",
    "                    pdf_text.append(text.strip())\n",
    "            return \" \".join(pdf_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\""
   ],
   "id": "60adb9ff6bb2e0bd",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:38:58.346184500Z",
     "start_time": "2025-03-13T04:36:14.442526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_keywords(text, top_n=5):\n",
    "    \"\"\"Extracts top N keywords based on word frequency.\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    common_words = Counter(words).most_common(top_n)\n",
    "    return [word for word, _ in common_words]"
   ],
   "id": "f892b31b6cd08691",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:38:58.347186200Z",
     "start_time": "2025-03-13T04:36:14.499515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(pdf_file):\n",
    "    \"\"\"Preprocesses PDF text and builds FAISS in memory (no saving).\"\"\"\n",
    "    pdf_text = get_text(pdf_file)\n",
    "    \n",
    "    keywords = extract_keywords(pdf_text, top_n=10)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "    docs = splitter.split_text(pdf_text)\n",
    "    documents = [Document(page_content=text) for text in docs if any(kw in text.lower() for kw in keywords)]\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    \n",
    "    text_embeddings_zip = list(zip(texts, embeddings))\n",
    "    \n",
    "    vector_db = FAISS.from_embeddings(text_embeddings_zip, embedding_model)\n",
    "    return vector_db"
   ],
   "id": "89288a65dc151a23",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:38:58.347186200Z",
     "start_time": "2025-03-13T04:36:14.543252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_llm(question, pdf_file):\n",
    "    vector_db = preprocess(pdf_file)\n",
    "    \n",
    "    print(\"Question: \", question)\n",
    "    \n",
    "    # Encode the question into an embedding\n",
    "    question_embedding = embedding_model.encode([question])[0]\n",
    "    \n",
    "    # Use the correct method to query the FAISS index\n",
    "    relevant_docs = vector_db.similarity_search_by_vector(question_embedding, k=5)\n",
    "    \n",
    "    context = \" \".join([doc.page_content for doc in relevant_docs])\n",
    "    keywords = extract_keywords(context, top_n=7)\n",
    "    filtered_sentences = [sent for sent in context.split(\". \") if any(kw in sent.lower() for kw in keywords)]\n",
    "    refined_context = \" \".join(filtered_sentences)\n",
    "    \n",
    "    if not refined_context.strip():\n",
    "        refined_context = context  \n",
    "    \n",
    "    prompt = f\"Summarize the following text in 2-3 sentences: {refined_context}\"\n",
    "    output = summarization_model(prompt, max_length=200, min_length=40, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    return re.sub(r'\\s+', ' ', output).strip()"
   ],
   "id": "4db8c449801f6b19",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-03-13T04:38:58.348187400Z",
     "start_time": "2025-03-13T04:36:14.596208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"What is the significance of using a unified representation in the 'One Model for All' framework?\",\n",
    "    \"How does the 'One Model for All' framework handle tasks with limited data?\",\n",
    "    \"What role do modality-specific sub-networks play in the 'One Model for All' framework?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"Answer:\", ask_llm(question, \"test-1.pdf\"))"
   ],
   "id": "306cb333ea20ff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cdae8d1af2f71194"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
