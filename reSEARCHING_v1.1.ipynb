{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T04:12:18.171142Z",
     "start_time": "2025-03-13T04:12:18.163778Z"
    }
   },
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:12:18.570311Z",
     "start_time": "2025-03-13T04:12:18.563813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_text(pdf_file):\n",
    "    try:\n",
    "        with open(pdf_file, 'rb') as f:\n",
    "            pdfReader = PyPDF2.PdfReader(f, strict=False)\n",
    "            pdf_text = []\n",
    "\n",
    "            for page in pdfReader.pages:\n",
    "                text = page.extract_text()\n",
    "\n",
    "                for line in text.split('\\n'):\n",
    "                    if len(line) > 5:\n",
    "                        line = re.sub(r'[^\\x20-\\x7E]', ' ', line)\n",
    "                        line = re.sub(r'\\s+', ' ', line).strip()\n",
    "                        pdf_text.append(line)\n",
    "\n",
    "            return \" \".join(pdf_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return \"\""
   ],
   "id": "60adb9ff6bb2e0bd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:12:20.723970Z",
     "start_time": "2025-03-13T04:12:18.905808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def truncate_text(text, max_tokens=1000):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)"
   ],
   "id": "f892b31b6cd08691",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:12:20.741124Z",
     "start_time": "2025-03-13T04:12:20.734978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(pdf_file):\n",
    "    pdf_text = get_text(pdf_file)\n",
    "    pdf_text = truncate_text(pdf_text)\n",
    "    \n",
    "    print(\"Text: \", pdf_text, \"\\n\\n\\n\")\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "    docs = splitter.split_text(pdf_text)\n",
    "    \n",
    "    print(\"Docs: \", docs, \"\\n\\n\\n\")\n",
    "    \n",
    "    documents = [Document(page_content=text) for text in docs]\n",
    "    vector_db = FAISS.from_documents(documents, embedding_model)\n",
    "    \n",
    "    retriever = vector_db.as_retriever()\n",
    "    return retriever\n"
   ],
   "id": "89288a65dc151a23",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:12:20.770163Z",
     "start_time": "2025-03-13T04:12:20.765070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_llm(question, pdf_file):\n",
    "    ret = preprocess(pdf_file)\n",
    "    llm = pipeline(\"summarization\", model=\"google/flan-t5-base\") \n",
    "\n",
    "    doc = ret.invoke(question)  \n",
    "\n",
    "    context = \"\\n\".join([d.page_content for d in doc]) \n",
    "    prompt = f'Summarize the given information: {context}'\n",
    "    output = llm(prompt, max_length=101)[0]['summary_text']\n",
    "    \n",
    "    return output"
   ],
   "id": "d4fdddf0f377cd93",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T04:12:29.190397Z",
     "start_time": "2025-03-13T04:12:20.858539Z"
    }
   },
   "cell_type": "code",
   "source": "print(ask_llm(\"What is this ?\", \"test-1.pdf\"))",
   "id": "306cb333ea20ff0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  One Model To Learn Them All ukasz Kaiser Google Brain lukaszkaiser@google.comAidan N. Gomez University of Toronto aidan@cs.toronto.eduNoam Shazeer Google Brain noam@google.com Ashish Vaswani Google Brain avaswani@google.comNiki Parmar Google Research nikip@google.comLlion Jones Google Research llion@google.comJakob Uszkoreit Google Research usz@google.com Abstract Deep learning yields great results across many elds, from speech recognition, image classi cation, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems span- ning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incor- porates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data bene t largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. 1 Introduction Recent successes of deep neural networks have spanned many domains, from computer vision [ 13] to speech recognition [ 8] and many other tasks. Convolutional networks excel at tasks related to vision, while recurrent neural networks have proven successful at natural language processing tasks, e.g., at machine translation [ 27,3,4]. But in each case, the network was designed and tuned speci cally for the problem at hand. This limits the impact of deep learning, as this effort needs to be repeated for each new task. It is also very different from the general nature of the human brain, which is able to learn many different tasks and bene t from transfer learning. The natural question arises: Can we create a uni ed deep learning model to solve tasks across multiple domains? The question about multi-task models has been studied in many papers in the deep learning literature. Natural language processing models have been shown to bene t from a multi-task approach a long time ago [ 6], and recently machine translation models have even been shown to exhibit zero-shot learning when trained on multiple langauges [ 18]. Speech recognition has also been shown to bene t from multi-task training [ 24], as have some vision problems, such as facial landmark detection [ 31]. But all these models are trained on other tasks from the same domain : translation tasks are trained with other translation tasks, vision tasks with other vision tasks, speech tasks with other speech tasks. Multi-modal learning has been shown to improve learned representations in the unsupervised Work performed while at Google Brain. Code available at https://github.com/tensorflow/tensor2tensorarXiv:1706.05137v1 [cs.LG] 16 Jun 2017 Figure 1: Examples decoded from a single MultiModel trained jointly on 8 tasks. Red depicts a language modality while blue depicts a categorical modality. setting [ 20] and when used as a-priori known unrelated tasks [ 22]. But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. In this work, we take a step toward positively answering the above question by introducing the MultiModel architecture, a single deep-learning model that can simultaneously learn multiple tasks from various domains. Concretely, we train the MultiModel simultaneously on the following 8 corpora: (1) WSJ speech corpus [7] (2) ImageNet dataset [23] (3) COCO image captioning dataset [14] (4) WSJ parsing dataset [17] (5) WMT English-German translation corpus (6) The reverse of the above: German-English translation. (7) WMT English-French translation corpus (8) The reverse of the above: German-French translation. The model learns all of the \n",
      "\n",
      "\n",
      "\n",
      "Docs:  ['One Model To Learn Them All ukasz Kaiser Google Brain lukaszkaiser@google.comAidan N. Gomez University of Toronto aidan@cs.toronto.eduNoam Shazeer Google Brain noam@google.com Ashish Vaswani Google', 'Brain noam@google.com Ashish Vaswani Google Brain avaswani@google.comNiki Parmar Google Research nikip@google.comLlion Jones Google Research llion@google.comJakob Uszkoreit Google Research', 'llion@google.comJakob Uszkoreit Google Research usz@google.com Abstract Deep learning yields great results across many elds, from speech recognition, image classi cation, to translation. But for each', 'image classi cation, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields', 'of tuning. We present a single model that yields good results on a number of problems span- ning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple', 'is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incor- porates', 'task. Our model architecture incor- porates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational', 'layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts', 'for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data bene t largely from joint training with other tasks,', 't largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. 1 Introduction Recent successes of deep neural networks have spanned many domains,', 'deep neural networks have spanned many domains, from computer vision [ 13] to speech recognition [ 8] and many other tasks. Convolutional networks excel at tasks related to vision, while recurrent', 'excel at tasks related to vision, while recurrent neural networks have proven successful at natural language processing tasks, e.g., at machine translation [ 27,3,4]. But in each case, the network', '[ 27,3,4]. But in each case, the network was designed and tuned speci cally for the problem at hand. This limits the impact of deep learning, as this effort needs to be repeated for each new task. It', 'effort needs to be repeated for each new task. It is also very different from the general nature of the human brain, which is able to learn many different tasks and bene t from transfer learning. The', 'tasks and bene t from transfer learning. The natural question arises: Can we create a uni ed deep learning model to solve tasks across multiple domains? The question about multi-task models has been', 'The question about multi-task models has been studied in many papers in the deep learning literature. Natural language processing models have been shown to bene t from a multi-task approach a long', 'shown to bene t from a multi-task approach a long time ago [ 6], and recently machine translation models have even been shown to exhibit zero-shot learning when trained on multiple langauges [ 18].', 'when trained on multiple langauges [ 18]. Speech recognition has also been shown to bene t from multi-task training [ 24], as have some vision problems, such as facial landmark detection [ 31]. But', 'such as facial landmark detection [ 31]. But all these models are trained on other tasks from the same domain : translation tasks are trained with other translation tasks, vision tasks with other', 'other translation tasks, vision tasks with other vision tasks, speech tasks with other speech tasks. Multi-modal learning has been shown to improve learned representations in the unsupervised Work', 'learned representations in the unsupervised Work performed while at Google Brain. Code available at https://github.com/tensorflow/tensor2tensorarXiv:1706.05137v1 [cs.LG] 16 Jun 2017 Figure 1:', '[cs.LG] 16 Jun 2017 Figure 1: Examples decoded from a single MultiModel trained jointly on 8 tasks. Red depicts a language modality while blue depicts a categorical modality. setting [ 20] and when', 'a categorical modality. setting [ 20] and when used as a-priori known unrelated tasks [ 22]. But no competitive multi-task multi-modal model has been proposed, so the above question remains', 'has been proposed, so the above question remains unanswered. In this work, we take a step toward positively answering the above question by introducing the MultiModel architecture, a single', 'introducing the MultiModel architecture, a single deep-learning model that can simultaneously learn multiple tasks from various domains. Concretely, we train the MultiModel simultaneously on the', 'we train the MultiModel simultaneously on the following 8 corpora: (1) WSJ speech corpus [7] (2) ImageNet dataset [23] (3) COCO image captioning dataset [14] (4) WSJ parsing dataset [17] (5) WMT', 'dataset [14] (4) WSJ parsing dataset [17] (5) WMT English-German translation corpus (6) The reverse of the above: German-English translation. (7) WMT English-French translation corpus (8) The reverse', 'English-French translation corpus (8) The reverse of the above: German-French translation. The model learns all of the'] \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Brain learned representations in the unsupervised Work performed while at Google Brain. Code available at https://github.com/tensorflow/tenson2tensonarXiv:1706.05137v1 [cs.LG]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T00:56:51.509860Z",
     "start_time": "2025-03-12T00:56:51.493605Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b08a3de213b4e10f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
